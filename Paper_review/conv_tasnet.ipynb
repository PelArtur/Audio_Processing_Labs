{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b7f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from trainer import Trainer\n",
    "from conv_tasnet import ConvTasNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eed615",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.yml\", mode='r') as f:\n",
    "    opt = yaml.load(f,Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73563c3e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a03c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTasNetMixDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, split: str ='train', max_len: int = -1, sample_rate: int = 8000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the base folder containing 'train', 'valid', 'test' directories.\n",
    "            split (str): One of ['train', 'valid', 'test']\n",
    "            sample_rate (int): Sampling rate for loading audio\n",
    "            segment (float or None): Duration of segment to crop (in seconds), or None for full audio\n",
    "        \"\"\"\n",
    "        self.mix_dir = os.path.join(root_dir, split, 'mix')\n",
    "        self.s1_dir = os.path.join(root_dir, split, 's1')\n",
    "        self.s2_dir = os.path.join(root_dir, split, 's2')\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.filenames = sorted(os.listdir(self.mix_dir))\n",
    "        if max_len != -1:\n",
    "            self.filenames = self.filenames[:max_len]\n",
    "        assert all(os.path.exists(os.path.join(self.s1_dir, f.replace(\"mix\", \"s1\"))) for f in self.filenames), \"Missing s1 files\"\n",
    "        assert all(os.path.exists(os.path.join(self.s2_dir, f.replace(\"mix\", \"s2\"))) for f in self.filenames), \"Missing s2 files\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mix_path = os.path.join(self.mix_dir, self.filenames[idx])\n",
    "        s1_path = os.path.join(self.s1_dir, self.filenames[idx].replace(\"mix\", \"s1\"))\n",
    "        s2_path = os.path.join(self.s2_dir, self.filenames[idx].replace(\"mix\", \"s2\"))\n",
    "\n",
    "        mix, mix_sr = torchaudio.load(mix_path)\n",
    "        if mix_sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(mix_sr, self.sample_rate)\n",
    "            mix = resampler(mix)\n",
    "\n",
    "        s1, s1_sr = torchaudio.load(s1_path)\n",
    "        if s1_sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(s1_sr, self.sample_rate)\n",
    "            s1 = resampler(s1)\n",
    "    \n",
    "        s2, s2_sr = torchaudio.load(s2_path)\n",
    "        if s2_sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(s2_sr, self.sample_rate)\n",
    "            s2 = resampler(s2)\n",
    "\n",
    "        mix = mix.mean(dim=0)\n",
    "        s1 = s1.mean(dim=0)\n",
    "        s2 = s2.mean(dim=0)\n",
    "\n",
    "        return {\n",
    "            \"mix\": mix, \n",
    "            \"ref\": [s1, s2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3984d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ConvTasNetMixDataset(root_dir=\"mix_dataset\", split=\"train\", sample_rate=8000)\n",
    "val_set   = ConvTasNetMixDataset(root_dir=\"mix_dataset\", split=\"valid\", sample_rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c8427",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16243328",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=3, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc919740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 01:37:41 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:153 - INFO ] Create optimizer adam: {'lr': 0.001, 'weight_decay': 1e-05}\n",
      "/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2025-05-18 01:37:41 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:121 - INFO ] Starting preparing model ............\n",
      "2025-05-18 01:37:41 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:122 - INFO ] Loading model to GPUs:(0,), #param: 3.48M\n",
      "2025-05-18 01:37:41 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:127 - INFO ] Gradient clipping by 200, default L2\n"
     ]
    }
   ],
   "source": [
    "model = ConvTasNet(**opt['net_conf']).to('cuda')\n",
    "trainer = Trainer(model, **opt['train'], resume=opt['resume'], optimizer_kwargs=opt['optimizer_kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db278020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 01:37:41 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:210 - INFO ] Validation model ......\n",
      "2025-05-18 01:38:06 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:229 - INFO ] <epoch:  0, lr:1.000e-03, loss:21.710, Total time:0.417 min> \n",
      "2025-05-18 01:38:06 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:242 - INFO ] Starting epoch from 0, loss = 21.7099\n",
      "2025-05-18 01:38:06 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:179 - INFO ] Training model ......\n",
      "2025-05-18 01:39:19 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:200, lr:1.000e-03, loss:0.771, batch:200 utterances> \n",
      "2025-05-18 01:40:31 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:400, lr:1.000e-03, loss:-0.071, batch:400 utterances> \n",
      "2025-05-18 01:41:42 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:600, lr:1.000e-03, loss:-0.416, batch:600 utterances> \n",
      "2025-05-18 01:42:53 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:800, lr:1.000e-03, loss:-0.386, batch:800 utterances> \n",
      "2025-05-18 01:44:05 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:1000, lr:1.000e-03, loss:-0.582, batch:1000 utterances> \n",
      "2025-05-18 01:45:17 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:1200, lr:1.000e-03, loss:-0.620, batch:1200 utterances> \n",
      "2025-05-18 01:46:29 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:1400, lr:1.000e-03, loss:-0.847, batch:1400 utterances> \n",
      "2025-05-18 01:47:41 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:197 - INFO ] <epoch:  1, iter:1600, lr:1.000e-03, loss:-0.731, batch:1600 utterances> \n",
      "2025-05-18 01:48:04 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:201 - INFO ] <epoch:  1, lr:1.000e-03, loss:-0.390, Total time:9.971 min> \n",
      "2025-05-18 01:48:04 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:210 - INFO ] Validation model ......\n",
      "2025-05-18 01:48:29 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:229 - INFO ] <epoch:  1, lr:1.000e-03, loss:-0.729, Total time:0.413 min> \n",
      "2025-05-18 01:48:29 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:262 - INFO ] Epoch: 1, now best loss change: -0.7293\n",
      "2025-05-18 01:48:29 [/home/artur/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:179 - INFO ] Training model ......\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:249\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 249\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval(val_dataloader)\n\u001b[1;32m    252\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/files/Audio Processing/Audio_Processing_Labs/Paper_review/trainer.py:193\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dataloader)\u001b[0m\n\u001b[1;32m    191\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_norm)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 193\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(losses)\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    195\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    196\u001b[0m         losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging_period:])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging_period\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0e3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./Conv-TasNet-new/best.pt\", map_location='cuda')\n",
    "model = ConvTasNet(N=512, L=16, B=128, H=512, P=3, X=8, R=3, num_spks=2, norm='gln')\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9234b",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc48547",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set  = ConvTasNetMixDataset(root_dir=\"mix_dataset\", split=\"test\", sample_rate=8000, max_len=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
